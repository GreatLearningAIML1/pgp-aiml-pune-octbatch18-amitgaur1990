{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Project2_Dog_Breed_Classification_Questions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2kIWaR5ZpKlJ"
      },
      "source": [
        "## Dog Breed Classification\n",
        "\n",
        "In this project we will use traditional CNN, CNN with data augmentation and finally transfer Learning by VGG16 model with weights pre-trained on Imagenet to solve the dog breed classification problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4Feze8OFfJX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86779657-e6c4-4bb7-e69f-86b3a77022bf"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import zipfile as zf\n",
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalMaxPooling2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F7MDmaAw2xGO"
      },
      "source": [
        "### Load Dataset Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BZWpQv1OwqYK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6784c90e-3dfe-4055-9192-c8adf1e3d1fb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fVhB9OopxFbX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "170732c6-5c41-43cc-f4da-ddb7f22f0f59"
      },
      "source": [
        "%cd /gdrive/'My Drive/Data/CNN2'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/Data/CNN2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1q2zzIaUprk_"
      },
      "source": [
        "Now, upload the given dataset file shared with you in your google drive and give its path for the below given `project_path` variable. For example, a path is given below according to the file path in our google drive. You need to change this to match the path of yours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3350WZM4w4EL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "994b35ef-f73c-4c1a-b760-8515ca3272ae"
      },
      "source": [
        "project_path"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"/gdrive/'My Drive/Data/CNN2\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tp6FvAToxUFs",
        "colab": {}
      },
      "source": [
        "project_path = \"/gdrive/'My Drive/Data/CNN2\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXZxAliwF-Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = zf.ZipFile('train.zip')\n",
        "file.extractall()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NHq1iBCfFjE",
        "colab_type": "text"
      },
      "source": [
        "Repeat the same step for test.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fxzynvB2YCb",
        "colab": {}
      },
      "source": [
        "file1 = zf.ZipFile('test.zip')\n",
        "file1.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnUMhQrDfJmz",
        "colab_type": "text"
      },
      "source": [
        "Repeat the same step for sample_submission.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4PyTxE8q2jLf",
        "colab": {}
      },
      "source": [
        "file = zf.ZipFile('sample_submission.csv.zip')\n",
        "file.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G9RIxB-fOLT",
        "colab_type": "text"
      },
      "source": [
        "Repeat the same step for labels.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rXtnEoEixbgi",
        "colab": {}
      },
      "source": [
        "file = zf.ZipFile('labels.csv.zip')\n",
        "file.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJc1lVrW_jmL",
        "colab_type": "text"
      },
      "source": [
        "After this process, we will have 4 files - Train folder, test folder and labels.csv and sample_submission.csv as part of your google drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aYmJKmDqqpng"
      },
      "source": [
        "### Read labels.csv file using pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmlJ2VMY96IZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = pd.read_csv(\"labels.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QP8YAzQvqyK-"
      },
      "source": [
        "### Print the count of each category of Dogs given in the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L2naXlr96Im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breed_counts = labels['breed'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dstKvSG7Lzc7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2fa3a794-10d0-4590-f55b-9ef4393e6fe8"
      },
      "source": [
        "breed_counts.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scottish_deerhound      126\n",
              "maltese_dog             117\n",
              "afghan_hound            116\n",
              "entlebucher             115\n",
              "bernese_mountain_dog    114\n",
              "Name: breed, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLm3W5RN96Ir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd109273-b5c7-4911-bb1c-81cc4cc2d06e"
      },
      "source": [
        "breed_counts.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(120,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WI94_Qcc0D4M"
      },
      "source": [
        "### Get one-hot encodings of labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q48iAcY196I3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "targets = pd.Series(labels['breed'])\n",
        "one_hot = pd.get_dummies(targets,sparse=True)\n",
        "one_hot_label = np.asarray(one_hot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nlWmRNM96I8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "796bc8c1-eb74-4238-d360-01bd2fb7928b"
      },
      "source": [
        "one_hot_label[0]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWaJ9naXfoiU",
        "colab_type": "text"
      },
      "source": [
        "## Preparing training dataset\n",
        "1. Write a code which reads each and every id from labels.csv file and loads the corresponding image (in RGB - 128, 128, 3) from the train folder. <br>\n",
        "2. Create 2 variables <br> \n",
        "     a.  x_train - Should have all the images of the dogs from train folder <br>\n",
        "     b.  y_train - Corresponding label of the dog <br>\n",
        "<u>Note:</u> The id of the dog images and its corresponding labels are available in labels.csv file   \n",
        "<u>Hint:</u> Watch the video shared on \"Preparing the training dataset\" if you face issue on creating the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6En4WJ6NIu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows =128\n",
        "img_columns= 128\n",
        "num_channels =1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aC2f9ecR0XGR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9ab0f8a-501a-4deb-bbb1-7d641ff8d209"
      },
      "source": [
        "import cv2\n",
        "from tqdm import tqdm\n",
        "x_train = []\n",
        "y_train =[]\n",
        "\n",
        "for f,img in tqdm(labels.values):\n",
        "  train_img =cv2.imread('./train/{}.jpg'.format(f),1)\n",
        "  train_img_resize =cv2.resize(train_img,(img_rows,img_columns))\n",
        "  x_train.append(train_img_resize)\n",
        "  y_train.append(img)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10222/10222 [01:11<00:00, 142.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ioWDEgElBOs",
        "colab_type": "text"
      },
      "source": [
        "Normalize the training data and convert into 4 dimensions so that it can be used as an input to conv layers in the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nkkZEpOe0ipk",
        "colab": {}
      },
      "source": [
        "def resize_images(img):\n",
        "  img = np.array(img).astype(np.uint8)\n",
        "  res = cv2.resize(img,(256,256), interpolation = cv2.INTER_CUBIC)\n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0ZCV0TISTrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = [resize_images(img) for img in x_train]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ARn76j3U1CDa",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "y = LabelBinarizer().fit_transform(y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bdCXuAE11gZL"
      },
      "source": [
        "### Split the training and validation data from `x_train_data` and `y_train_data` obtained from above step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpWx-pgV96Jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_train, y, test_size=0.3, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq3HY4bKRiBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test, X_val, y_test, Y_val = train_test_split(X_test,y_test, test_size = 0.5, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XkL-N1jDsU8m"
      },
      "source": [
        "### Loading the test data\n",
        "Read the id column from the samples_submission.csv and store it in test_img"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DnpXdpd9b3E7",
        "colab": {}
      },
      "source": [
        "test_img = pd.read_csv('sample_submission.csv').id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcBKhlmbR6t5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d26594d3-b4ea-4485-bbe1-c38387c6d73f"
      },
      "source": [
        "test_img.head()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    000621fb3cbb32d8935728e48679680e\n",
              "1    00102ee9d8eb90812350685311fe5890\n",
              "2    0012a730dfa437f5f3613fb75efcd4ce\n",
              "3    001510bc8570bbeee98c8d80c8a95ec1\n",
              "4    001a5f3114548acdefa3d4da05474c2e\n",
              "Name: id, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEJqZIMbm0Jo",
        "colab_type": "text"
      },
      "source": [
        "Run the below code to load the test image files in x_test_feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zf7n4WG-b3Hv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fd04a2c-2ac6-40b9-feb3-056970305f06"
      },
      "source": [
        "x_test_feature = []\n",
        "i = 0 # initialisation\n",
        "for f in tqdm(test_img.values): # f for format ,jpg\n",
        "    img = cv2.imread('./test/{}.jpg'.format(f), 0)\n",
        "    img_resize = cv2.resize(img, (img_rows, img_columns)) \n",
        "    x_test_feature.append(img_resize)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10357/10357 [01:00<00:00, 171.91it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9My6qSyDnE-_",
        "colab_type": "text"
      },
      "source": [
        "Normalize the test data and convert it into 4 dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93n-IntMnJGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test_feature = [resize_images(img) for img in x_test_feature]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zKezNJVMsocP"
      },
      "source": [
        "### Build a basic conv neural network with 2 conv layers (kernel sizes - 5 and 3) add layers as mentioned below for classification.\n",
        "\n",
        "1. Add a Dense layer with 256 neurons with `relu` activation\n",
        "\n",
        "2. Add a Dense layer with 120 neurons as final layer (as there are 120 classes in the given dataset) with `softmax` activation for classifiaction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2jxTY2S96J4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5), activation ='relu', batch_input_shape = (5,256, 256, 3)))\n",
        "\n",
        "model.add(Conv2D(filters = 32, kernel_size = (3,3), activation ='relu'))\n",
        "\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "\n",
        "model.add(Dense(120, activation = \"softmax\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_BAvCzo96J6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "83878add-e92a-4213-a5d6-9ad001b05663"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (5, 252, 252, 32)         2432      \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (5, 250, 250, 32)         9248      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (5, 250, 250, 256)        8448      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (5, 250, 250, 120)        30840     \n",
            "=================================================================\n",
            "Total params: 50,968\n",
            "Trainable params: 50,968\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui8EXw6_oqpR",
        "colab_type": "text"
      },
      "source": [
        "### Use batch_size = 128 and epochs = 10 and execute the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg5Vc1JdUOqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = 'Adam', loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IriIc37NozbK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "36b6f688-6bf3-45b5-f602-95155052eb2a"
      },
      "source": [
        "import numpy as np\n",
        "model.fit(np.array(X_train), np.array(y_train), epochs = 10, validation_data = (np.array(X_val),np.array(Y_val)),batch_size = 120)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-312d9f58fc64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_20 to have 4 dimensions, but got array with shape (7155, 120)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PR9j5_Xozmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8hWaKmjoz69",
        "colab_type": "text"
      },
      "source": [
        "#The model accuracy is very poor !!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "agJKkc6xtKiq"
      },
      "source": [
        "### Use Data Augmentation in the above model to see if the accuracy improves\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "31Mn8qnZb3Ru",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDLQVFDP96KI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bqTlW0qHb3Xb",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sssbaTfxlkk"
      },
      "source": [
        "### Using the above objects, create the image generators with variable names `train_generator` and `val_generator`\n",
        "\n",
        "You need to use train_datagen.flow() and val_datagen.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sehaRgT-96KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TVFQJZw3x4-C"
      },
      "source": [
        "### Fit the model using fit_generator() using `train_generator` and `val_generator` from the above step with 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J1K2MqHbuPUa",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2zmLztqo5DY",
        "colab_type": "text"
      },
      "source": [
        "# Model accuracy is still poor!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSTATrhsAo7L",
        "colab_type": "text"
      },
      "source": [
        "### Lets use Transfer Learning\n",
        "\n",
        "Download the vgg wieght file from here : https://github.com/MinerKasch/applied_deep_learning/blob/master/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy5JdbW6pIvD",
        "colab_type": "text"
      },
      "source": [
        "Use the below code to load VGG16 weights trained on ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrqs0zg7ApNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "# Instantiate the model with the pre-trained weights (no top)\n",
        "base_model= VGG16(weights=(project_path+'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'),\n",
        "                 include_top=False, pooling='avg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EItOlRBGpV_A",
        "colab_type": "text"
      },
      "source": [
        "Print the summary of the base_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQsEBgnlpHjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHpeOyW0qauW",
        "colab_type": "text"
      },
      "source": [
        "### Add the following classification layers to the imported VGG Model <br>\n",
        "1. Flatten Layer\n",
        "2. Dense layer with 1024 neurons with activation as Relu\n",
        "3. Dense layer with 256 neurons with activation as Relu\n",
        "4. Dense layer with 120 neurons with activation as Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BpT4MLkqoaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeQem0pHITIj",
        "colab_type": "text"
      },
      "source": [
        "### Make all the layers in the base_model (VGG16) to be non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7w9CSPvIRnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj-BwqgfIkdv",
        "colab_type": "text"
      },
      "source": [
        "### Fit and compile the model with batch_size = 128 and epochs = 10 and execute the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD5fAgVQIpKZ",
        "colab_type": "text"
      },
      "source": [
        "Try to get training and validation accuracy to be more than 90%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZk2SWvjIoRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}