{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R8_Internal_Lab_Questions_Transfer_Learning_MNIST_Tweet_Sentiment_Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "9qU14lYL9A5g",
        "z78o3WIjaEJ3",
        "nAQDiZHRH0M_",
        "jPJvTjefH0NI",
        "Q5nlCuaaH0OD"
      ]
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NFfDTfhlaEI_"
      },
      "source": [
        "# Transfer Learning MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rNwbqCFRaEJC"
      },
      "source": [
        "* Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n",
        "* Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YUB1uDW_8XIy"
      },
      "source": [
        "## 1. Import necessary libraries for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rsj4t5HTaEJE",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IXrn3heBaEJa"
      },
      "source": [
        "## 2. Import MNIST data and create 2 datasets with one dataset having digits from 0 to 4 and other from 5 to 9 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pjDuiK6ztgOK",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# create two datasets one with digits below 5 and one with 5 and above\n",
        "x_train_lt5 = x_train[y_train < 5]\n",
        "y_train_lt5 = y_train[y_train < 5]\n",
        "x_test_lt5 = x_test[y_test < 5]\n",
        "y_test_lt5 = y_test[y_test < 5]\n",
        "\n",
        "x_train_gte5 = x_train[y_train >= 5]\n",
        "y_train_gte5 = y_train[y_train >= 5] - 5\n",
        "x_test_gte5 = x_test[y_test >= 5]\n",
        "y_test_gte5 = y_test[y_test >= 5] - 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9qU14lYL9A5g"
      },
      "source": [
        "## 3. Print x_train, y_train, x_test and y_test for both the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9OrszhJ0SgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "395da588-8a08-4990-a690-cda195c299cf"
      },
      "source": [
        "x_train_lt5"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sJswV4xk9jQS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d1f6050-e109-424c-9421-5971c0fa1348"
      },
      "source": [
        "y_train_lt5"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 4, 1, ..., 2, 1, 3], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UybJJRcFM5ey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "7aa57b34-c136-4452-8262-b4b0a3ed02ef"
      },
      "source": [
        "x_test_lt5"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDM-Khl1M5mc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "027baabe-911b-4236-ab26-f3083d8c3d5b"
      },
      "source": [
        "y_test_lt5"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 0, ..., 2, 3, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvM7FXPLM6Iq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "3bf21fe2-5b28-4c8b-e3bc-1c3e2ca13037"
      },
      "source": [
        "x_train_gte5"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZIwNPysM6Or",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39fbd909-1b23-4482-e3fb-d81d131e21a2"
      },
      "source": [
        "y_train_gte5"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 4, 0, ..., 0, 1, 3], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri_2FvnWM6WP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "ffe7f91d-d940-4a5f-f58b-5ece1d443739"
      },
      "source": [
        "x_test_gte5"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9R30KCkNBHI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4dc19c48-ebd5-4a61-d8d9-5365b9bea8c1"
      },
      "source": [
        "y_test_gte5"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 4, 0, ..., 4, 0, 1], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cB9BPFzr9oDF"
      },
      "source": [
        "## ** 4. Let us take only the dataset (x_train, y_train, x_test, y_test) for Integers 0 to 4 in MNIST **\n",
        "## Reshape x_train and x_test to a 4 Dimensional array (channel = 1) to pass it into a Conv2D layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FlQRPfFzaEJx",
        "colab": {}
      },
      "source": [
        "x_train_lt5_conv = x_train_lt5.reshape(x_train_lt5.shape[0], 28, 28, 1)\n",
        "x_test_lt5_conv = x_test_lt5.reshape(x_test_lt5.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VQVdCNpOC72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22c6bf55-0864-43f7-d2c8-1bb32f77f1ea"
      },
      "source": [
        "x_train_lt5_conv.shape"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30596, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jLQr-b3F-hw8"
      },
      "source": [
        "## 5. Normalize x_train and x_test by dividing it by 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PlEZIAG5-g2I",
        "colab": {}
      },
      "source": [
        "x_train_lt5_conv_norm =  x_train_lt5_conv.astype(\"float32\") / 255\n",
        "x_test_lt5_conv_norm = x_test_lt5_conv.astype(\"float32\") / 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pytVBaw4-vMi"
      },
      "source": [
        "## 6. Use One-hot encoding to divide y_train and y_test into required no of output classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V48xiua4-uUi",
        "colab": {}
      },
      "source": [
        "y_train_class = keras.utils.to_categorical(y_train_lt5, 10)\n",
        "y_test_class = keras.utils.to_categorical(y_test_lt5, 10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "elPkI44g_C2b"
      },
      "source": [
        "## 7. Build a sequential model with 2 Convolutional layers with 32 kernels of size (3,3) followed by a Max pooling layer of size (2,2) followed by a drop out layer to be trained for classification of digits 0-4  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67ZDZ4crN4d2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eucpx4HJN_OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1),name='conv1'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1),name='conv2'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),name='max1'))\n",
        "model.add(Dropout(0.25,name='drop1'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJQaycRO_3Au"
      },
      "source": [
        "## 8. Post that flatten the data and add 2 Dense layers with 128 neurons and neurons = output classes with activation = 'relu' and 'softmax' respectively. Add dropout layer inbetween if necessary  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOZeRbK7t9AT",
        "colab": {}
      },
      "source": [
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu',name='dense1'))\n",
        "model.add(Dense(10, activation='softmax',name='dense2'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "my1P09bxAv8H"
      },
      "source": [
        "## 9. Print the training and test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y4TsjrfO9lJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "4d4c53c9-6d22-45fa-dc8a-19dc82fc4ec6"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "output_simple_conv = model.fit(x_train_lt5_conv_norm, y_train_class, batch_size=512, epochs=10, verbose=2,\n",
        "                    validation_data=(x_test_lt5_conv_norm, y_test_class))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 30596 samples, validate on 5139 samples\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.3396 - acc: 0.9020 - val_loss: 0.0784 - val_acc: 0.9805\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.0670 - acc: 0.9792 - val_loss: 0.0299 - val_acc: 0.9907\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.0386 - acc: 0.9886 - val_loss: 0.0224 - val_acc: 0.9934\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.0281 - acc: 0.9913 - val_loss: 0.0170 - val_acc: 0.9947\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.0235 - acc: 0.9927 - val_loss: 0.0242 - val_acc: 0.9922\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0201 - acc: 0.9936 - val_loss: 0.0124 - val_acc: 0.9953\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0160 - acc: 0.9954 - val_loss: 0.0101 - val_acc: 0.9959\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0139 - acc: 0.9961 - val_loss: 0.0095 - val_acc: 0.9965\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0122 - acc: 0.9963 - val_loss: 0.0091 - val_acc: 0.9965\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0096 - acc: 0.9971 - val_loss: 0.0080 - val_acc: 0.9971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1X5cMe0Q4GZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "19adbe13-1c31-4529-c9b8-15f224aca520"
      },
      "source": [
        "score = model.evaluate(x_test_lt5_conv_norm, y_test_class, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Test score:', 0.0076241770370868)\n",
            "('Test accuracy:', 0.9972757345787118)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z78o3WIjaEJ3"
      },
      "source": [
        "## 10. Make only the dense layers to be trainable and convolutional layers to be non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "brN7VZHFaEJ4",
        "colab": {}
      },
      "source": [
        "for layer in model.layers:\n",
        "  if('dense' not in layer.name): #prefix detection to freeze layers which does not have dense\n",
        "    layer.trainable = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4opnW7o0BJ8P"
      },
      "source": [
        "## 11. Use the model trained on 0 to 4 digit classification and train it on the dataset which has digits 5 to 9  (Using Transfer learning keeping only the dense layers to be trainable)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lCFcYHTm6-cE",
        "colab": {}
      },
      "source": [
        "y_train_class = keras.utils.to_categorical(y_train_gte5, 10)\n",
        "y_test_class = keras.utils.to_categorical(y_test_gte5, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAFE2uViUPVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_gte5_conv = x_train_gte5.reshape(x_train_gte5.shape[0], 28, 28, 1)\n",
        "x_test_gte5_conv = x_test_gte5.reshape(x_test_gte5.shape[0], 28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qe4OaE1UdCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_gte5_conv_norm =  x_train_gte5_conv.astype(\"float32\") / 255\n",
        "x_test_gte5_conv_norm = x_test_gte5_conv.astype(\"float32\") / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DITyAt3t7Tto",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "c524fefc-5f54-42a2-89e5-337b32eb6621"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "output_simple_conv = model.fit(x_train_gte5_conv_norm, y_train_class, batch_size=512, epochs=10, verbose=2,\n",
        "                    validation_data=(x_test_gte5_conv_norm, y_test_class))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 29404 samples, validate on 4861 samples\n",
            "Epoch 1/10\n",
            " - 1s - loss: 2.4428 - acc: 0.6897 - val_loss: 0.1941 - val_acc: 0.9475\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1359 - acc: 0.9609 - val_loss: 0.1070 - val_acc: 0.9712\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0903 - acc: 0.9750 - val_loss: 0.0771 - val_acc: 0.9774\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0701 - acc: 0.9806 - val_loss: 0.0612 - val_acc: 0.9819\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0584 - acc: 0.9836 - val_loss: 0.0516 - val_acc: 0.9850\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0508 - acc: 0.9852 - val_loss: 0.0453 - val_acc: 0.9864\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0450 - acc: 0.9872 - val_loss: 0.0405 - val_acc: 0.9883\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0405 - acc: 0.9890 - val_loss: 0.0384 - val_acc: 0.9885\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0370 - acc: 0.9893 - val_loss: 0.0353 - val_acc: 0.9891\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0346 - acc: 0.9899 - val_loss: 0.0338 - val_acc: 0.9895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SoDozqghCJZ4"
      },
      "source": [
        "## 12. Print the accuracy for classification of digits 5 to 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9fCxgb5s49Cj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b9ae585d-29ea-45c0-8fd3-08ca15372002"
      },
      "source": [
        "score = model.evaluate(x_test_gte5_conv_norm, y_test_class, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Test score:', 0.03376136876192849)\n",
            "('Test accuracy:', 0.9895083315086521)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU-HwvIdH0M-",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment analysis <br> \n",
        "\n",
        "The objective of the second problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
        "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAQDiZHRH0M_",
        "colab_type": "text"
      },
      "source": [
        "### 13. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PIfvpAuVgbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "075cc1c0-e058-4e39-9bf7-296a4c43e44d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eXGIe-SH0NA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/gdrive/My Drive/tweets.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPtn_mczWqhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWeWe1eJH0NF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdb5ee2d-922b-40ad-dc05-2aa96e00a0b2"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3291, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPJvTjefH0NI",
        "colab_type": "text"
      },
      "source": [
        "### 14. Preprocess the text and add the preprocessed text in a column with name `text` in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iec5s9gH0NI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text):\n",
        "    try:\n",
        "        return text.decode('ascii')\n",
        "    except Exception as e:\n",
        "        return \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntjqihSmWwHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['text'] = [preprocess(text) for text in df.tweet_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGWB3P2WH0NY",
        "colab_type": "text"
      },
      "source": [
        "### 15. Consider only rows having Positive emotion and Negative emotion and remove other rows from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdgA_8N2H0NY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[ (df[\"is_there_an_emotion_directed_at_a_brand_or_product\"] == \"Positive emotion\") | (df[\"is_there_an_emotion_directed_at_a_brand_or_product\"] == \"Negative emotion\")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jlu-reIH0Na",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5bc74e35-e4e9-46b6-e191-df8013439172"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3191, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SotCRvkDH0Nf",
        "colab_type": "text"
      },
      "source": [
        "### 16. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
        "\n",
        "#### Use `vect` as the variable name for initialising CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcbkY4sgH0Ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer(analyzer=\"word\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyXtZGr-H0Nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = vect.fit_transform(df.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4LUM-XPH0Nn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a6c119f-adca-443e-ab74-6731d4849043"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3191, 5482)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pxd5fSHH0Nt",
        "colab_type": "text"
      },
      "source": [
        "### 17. Find number of different words in vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1DQ2LdNH0Nu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17034
        },
        "outputId": "ebf8dc12-b25e-40f0-ca91-d366385f448e"
      },
      "source": [
        "vect.get_feature_names()"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'000',\n",
              " u'02',\n",
              " u'03',\n",
              " u'08',\n",
              " u'10',\n",
              " u'100',\n",
              " u'100s',\n",
              " u'100tc',\n",
              " u'101',\n",
              " u'10am',\n",
              " u'10k',\n",
              " u'10mins',\n",
              " u'10pm',\n",
              " u'10x',\n",
              " u'11',\n",
              " u'11ntc',\n",
              " u'11th',\n",
              " u'12',\n",
              " u'12b',\n",
              " u'12th',\n",
              " u'13',\n",
              " u'130',\n",
              " u'14',\n",
              " u'1406',\n",
              " u'1413',\n",
              " u'1415',\n",
              " u'15',\n",
              " u'150',\n",
              " u'1500',\n",
              " u'150m',\n",
              " u'157',\n",
              " u'15am',\n",
              " u'15k',\n",
              " u'16162',\n",
              " u'16gb',\n",
              " u'16mins',\n",
              " u'17',\n",
              " u'188',\n",
              " u'1986',\n",
              " u'1990style',\n",
              " u'1m',\n",
              " u'1pm',\n",
              " u'1st',\n",
              " u'20',\n",
              " u'200',\n",
              " u'2010',\n",
              " u'2011',\n",
              " u'2012',\n",
              " u'20s',\n",
              " u'21',\n",
              " u'22',\n",
              " u'23',\n",
              " u'24',\n",
              " u'25',\n",
              " u'250k',\n",
              " u'25th',\n",
              " u'2am',\n",
              " u'2day',\n",
              " u'2honor',\n",
              " u'2moro',\n",
              " u'2nd',\n",
              " u'2nite',\n",
              " u'2s',\n",
              " u'2yrs',\n",
              " u'30',\n",
              " u'300',\n",
              " u'3000',\n",
              " u'30a',\n",
              " u'30am',\n",
              " u'30p',\n",
              " u'30pm',\n",
              " u'32',\n",
              " u'32gb',\n",
              " u'35',\n",
              " u'36',\n",
              " u'37',\n",
              " u'3d',\n",
              " u'3g',\n",
              " u'3gs',\n",
              " u'3k',\n",
              " u'3rd',\n",
              " u'3x',\n",
              " u'40',\n",
              " u'400',\n",
              " u'40min',\n",
              " u'41',\n",
              " u'45',\n",
              " u'45am',\n",
              " u'47',\n",
              " u'48',\n",
              " u'4android',\n",
              " u'4chan',\n",
              " u'4g',\n",
              " u'4nqv92l',\n",
              " u'4sq',\n",
              " u'4sq3',\n",
              " u'4square',\n",
              " u'50',\n",
              " u'54',\n",
              " u'55',\n",
              " u'58',\n",
              " u'59',\n",
              " u'59pm',\n",
              " u'5pm',\n",
              " u'5th',\n",
              " u'60',\n",
              " u'64g',\n",
              " u'64gb',\n",
              " u'64gig',\n",
              " u'64mb',\n",
              " u'65',\n",
              " u'6hours',\n",
              " u'6th',\n",
              " u'70',\n",
              " u'75',\n",
              " u'7th',\n",
              " u'80',\n",
              " u'800',\n",
              " u'80s',\n",
              " u'81',\n",
              " u'82',\n",
              " u'89',\n",
              " u'8am',\n",
              " u'8p',\n",
              " u'8pm',\n",
              " u'90',\n",
              " u'900',\n",
              " u'911tweets',\n",
              " u'95',\n",
              " u'96',\n",
              " u'967',\n",
              " u'97',\n",
              " u'98',\n",
              " u'99',\n",
              " u'9th',\n",
              " u'______',\n",
              " u'_______',\n",
              " u'a3xvwc6',\n",
              " u'aapl',\n",
              " u'abacus',\n",
              " u'abandoned',\n",
              " u'ability',\n",
              " u'able',\n",
              " u'about',\n",
              " u'abroad',\n",
              " u'absolute',\n",
              " u'absolutely',\n",
              " u'abt',\n",
              " u'abuzz',\n",
              " u'academy',\n",
              " u'acc',\n",
              " u'acceptable',\n",
              " u'access',\n",
              " u'accessibility',\n",
              " u'accessible',\n",
              " u'accessories',\n",
              " u'accessory',\n",
              " u'accesssxsw',\n",
              " u'accommodate',\n",
              " u'according',\n",
              " u'accordion',\n",
              " u'account',\n",
              " u'acerbic',\n",
              " u'achieve',\n",
              " u'acknowledge',\n",
              " u'aclu',\n",
              " u'aclus',\n",
              " u'acquired',\n",
              " u'across',\n",
              " u'acrosse',\n",
              " u'action',\n",
              " u'actions',\n",
              " u'activate',\n",
              " u'activations',\n",
              " u'activity',\n",
              " u'actors',\n",
              " u'actsofsharing',\n",
              " u'actual',\n",
              " u'actually',\n",
              " u'ad',\n",
              " u'adam',\n",
              " u'adapt',\n",
              " u'adaptive',\n",
              " u'add',\n",
              " u'added',\n",
              " u'addicted',\n",
              " u'addictedtotheinterwebs',\n",
              " u'addictive',\n",
              " u'addicts',\n",
              " u'adding',\n",
              " u'addition',\n",
              " u'additional',\n",
              " u'address',\n",
              " u'admired',\n",
              " u'admission',\n",
              " u'admit',\n",
              " u'admits',\n",
              " u'admitting',\n",
              " u'ado',\n",
              " u'adopter',\n",
              " u'adopters',\n",
              " u'adoption',\n",
              " u'adpeopleproblems',\n",
              " u'ads',\n",
              " u'advanced',\n",
              " u'advent',\n",
              " u'adventure',\n",
              " u'advertising',\n",
              " u'advice',\n",
              " u'advisory',\n",
              " u'aesthetic',\n",
              " u'affair',\n",
              " u'affirmative',\n",
              " u'afford',\n",
              " u'afraid',\n",
              " u'africans',\n",
              " u'after',\n",
              " u'afternoon',\n",
              " u'again',\n",
              " u'against',\n",
              " u'agchat',\n",
              " u'agencies',\n",
              " u'agency',\n",
              " u'agenda',\n",
              " u'agents',\n",
              " u'agileagency',\n",
              " u'agnerd',\n",
              " u'ago',\n",
              " u'agree',\n",
              " u'agreed',\n",
              " u'ah',\n",
              " u'ahead',\n",
              " u'ahem',\n",
              " u'ahh',\n",
              " u'ahhh',\n",
              " u'ahing',\n",
              " u'aicn',\n",
              " u'aiding',\n",
              " u'aim',\n",
              " u'ain',\n",
              " u'air',\n",
              " u'airline',\n",
              " u'airlines',\n",
              " u'airplane',\n",
              " u'airport',\n",
              " u'airports',\n",
              " u'airs',\n",
              " u'ajs2011',\n",
              " u'aka',\n",
              " u'akqas',\n",
              " u'al',\n",
              " u'alamo',\n",
              " u'alan',\n",
              " u'alarm',\n",
              " u'alarms',\n",
              " u'alas',\n",
              " u'alert',\n",
              " u'alerts',\n",
              " u'alex',\n",
              " u'alive',\n",
              " u'all',\n",
              " u'allow',\n",
              " u'allowing',\n",
              " u'almost',\n",
              " u'alone',\n",
              " u'along',\n",
              " u'alot',\n",
              " u'alphagraphics',\n",
              " u'already',\n",
              " u'also',\n",
              " u'alt',\n",
              " u'alternate',\n",
              " u'alternative',\n",
              " u'although',\n",
              " u'always',\n",
              " u'alwayshavingtoplugin',\n",
              " u'am',\n",
              " u'amateurhour',\n",
              " u'amazing',\n",
              " u'amazingly',\n",
              " u'amazon',\n",
              " u'ambassador',\n",
              " u'america',\n",
              " u'amex',\n",
              " u'amigos',\n",
              " u'among',\n",
              " u'amount',\n",
              " u'amp',\n",
              " u'amused',\n",
              " u'amusing',\n",
              " u'an',\n",
              " u'analysis',\n",
              " u'analytics',\n",
              " u'and',\n",
              " u'andoid',\n",
              " u'andriod',\n",
              " u'andro',\n",
              " u'android',\n",
              " u'androidsxsw',\n",
              " u'angry',\n",
              " u'angrybirds',\n",
              " u'announce',\n",
              " u'announced',\n",
              " u'announces',\n",
              " u'announcing',\n",
              " u'annoyed',\n",
              " u'annoying',\n",
              " u'another',\n",
              " u'answer',\n",
              " u'answered',\n",
              " u'anti',\n",
              " u'anticipate',\n",
              " u'antigov',\n",
              " u'antique',\n",
              " u'antonio',\n",
              " u'antwoord',\n",
              " u'anxiety',\n",
              " u'anxious',\n",
              " u'any',\n",
              " u'anybody',\n",
              " u'anybodywanttobuymeanipad2',\n",
              " u'anymore',\n",
              " u'anyone',\n",
              " u'anyones',\n",
              " u'anything',\n",
              " u'anyway',\n",
              " u'anyways',\n",
              " u'anywhere',\n",
              " u'aos',\n",
              " u'ap',\n",
              " u'apac',\n",
              " u'apartment',\n",
              " u'api',\n",
              " u'apis',\n",
              " u'app',\n",
              " u'apparent',\n",
              " u'apparently',\n",
              " u'appcircus',\n",
              " u'appeal',\n",
              " u'appealing',\n",
              " u'appear',\n",
              " u'appears',\n",
              " u'applauds',\n",
              " u'applause',\n",
              " u'apple',\n",
              " u'apple_store',\n",
              " u'appleaddiction',\n",
              " u'appleatxdt',\n",
              " u'applefanatic',\n",
              " u'apples',\n",
              " u'appletakingoverworld',\n",
              " u'application',\n",
              " u'applications',\n",
              " u'appolicious',\n",
              " u'appreciate',\n",
              " u'appreciation',\n",
              " u'approaches',\n",
              " u'approval',\n",
              " u'approved',\n",
              " u'approves',\n",
              " u'apps',\n",
              " u'aquent',\n",
              " u'arcade',\n",
              " u'archive',\n",
              " u'arctic',\n",
              " u'arduino',\n",
              " u'are',\n",
              " u'area',\n",
              " u'areas',\n",
              " u'aren',\n",
              " u'arg',\n",
              " u'argues',\n",
              " u'argument',\n",
              " u'aristotle',\n",
              " u'arm',\n",
              " u'armadillo',\n",
              " u'armed',\n",
              " u'aron',\n",
              " u'around',\n",
              " u'arrived',\n",
              " u'arrives',\n",
              " u'arriving',\n",
              " u'art',\n",
              " u'article',\n",
              " u'articles',\n",
              " u'articulate',\n",
              " u'artificial',\n",
              " u'artist',\n",
              " u'artistic',\n",
              " u'artists',\n",
              " u'artwork',\n",
              " u'artworks',\n",
              " u'arw',\n",
              " u'as',\n",
              " u'asddieu',\n",
              " u'ask',\n",
              " u'asked',\n",
              " u'asking',\n",
              " u'asleep',\n",
              " u'ass',\n",
              " u'assistivetech',\n",
              " u'assume',\n",
              " u'at',\n",
              " u'atari',\n",
              " u'atl',\n",
              " u'atms',\n",
              " u'atrix',\n",
              " u'att',\n",
              " u'attached',\n",
              " u'attempt',\n",
              " u'attend',\n",
              " u'attended',\n",
              " u'attendees',\n",
              " u'attending',\n",
              " u'attention',\n",
              " u'attitudes',\n",
              " u'attracted',\n",
              " u'attracting',\n",
              " u'attractive',\n",
              " u'atx',\n",
              " u'atzip',\n",
              " u'audience',\n",
              " u'audio',\n",
              " u'augcomm',\n",
              " u'augmented',\n",
              " u'augmentedreality',\n",
              " u'auntie',\n",
              " u'aus',\n",
              " u'austin',\n",
              " u'austincrowd',\n",
              " u'austinites',\n",
              " u'austintx',\n",
              " u'austinwins',\n",
              " u'australian',\n",
              " u'ausxsw',\n",
              " u'auth',\n",
              " u'authenticator',\n",
              " u'authorization',\n",
              " u'autistic',\n",
              " u'auto',\n",
              " u'autocorrect',\n",
              " u'autocorrects',\n",
              " u'autonomous',\n",
              " u'avail',\n",
              " u'available',\n",
              " u'ave',\n",
              " u'avenue',\n",
              " u'average',\n",
              " u'averages',\n",
              " u'avoid',\n",
              " u'avoiding',\n",
              " u'aw',\n",
              " u'awake',\n",
              " u'award',\n",
              " u'awards',\n",
              " u'aware',\n",
              " u'awareness',\n",
              " u'away',\n",
              " u'awe',\n",
              " u'awesome',\n",
              " u'awesomely',\n",
              " u'awesomeness',\n",
              " u'awesometiming',\n",
              " u'awhile',\n",
              " u'awkward',\n",
              " u'awwww',\n",
              " u'axzwxb',\n",
              " u'b4',\n",
              " u'baby',\n",
              " u'back',\n",
              " u'background',\n",
              " u'backlight',\n",
              " u'backpack',\n",
              " u'backup',\n",
              " u'backupify',\n",
              " u'bad',\n",
              " u'badge',\n",
              " u'badges',\n",
              " u'bag',\n",
              " u'bags',\n",
              " u'bahahahaha',\n",
              " u'bajillions',\n",
              " u'balance',\n",
              " u'balckberries',\n",
              " u'balcony',\n",
              " u'ball',\n",
              " u'ballroom',\n",
              " u'ballrooms',\n",
              " u'banality',\n",
              " u'band',\n",
              " u'bands',\n",
              " u'bandwaggoners',\n",
              " u'bandwidth',\n",
              " u'bang',\n",
              " u'banged',\n",
              " u'bank',\n",
              " u'banking',\n",
              " u'bankinnovate',\n",
              " u'bankinnovation',\n",
              " u'banks',\n",
              " u'bar',\n",
              " u'barcode',\n",
              " u'barely',\n",
              " u'barring',\n",
              " u'barroom',\n",
              " u'barry',\n",
              " u'barrydiller',\n",
              " u'bars',\n",
              " u'bart',\n",
              " u'barton',\n",
              " u'based',\n",
              " u'bashing',\n",
              " u'basic',\n",
              " u'basically',\n",
              " u'basics',\n",
              " u'basis',\n",
              " u'basket',\n",
              " u'bastards',\n",
              " u'bat',\n",
              " u'bathroom',\n",
              " u'batphone',\n",
              " u'batt',\n",
              " u'batteries',\n",
              " u'battery',\n",
              " u'batterykiller',\n",
              " u'battle',\n",
              " u'battledecks',\n",
              " u'battlela',\n",
              " u'bavcid',\n",
              " u'bawling',\n",
              " u'bb',\n",
              " u'bbq',\n",
              " u'bc',\n",
              " u'bday',\n",
              " u'be',\n",
              " u'beach',\n",
              " u'beans',\n",
              " u'bear',\n",
              " u'beard',\n",
              " u'beat',\n",
              " u'beats',\n",
              " u'beautiful',\n",
              " u'beautifully',\n",
              " u'beauty',\n",
              " u'because',\n",
              " u'become',\n",
              " u'becoming',\n",
              " u'bed',\n",
              " u'beechwood',\n",
              " u'been',\n",
              " u'beer',\n",
              " u'before',\n",
              " u'beforetwitter',\n",
              " u'begin',\n",
              " u'beginning',\n",
              " u'begins',\n",
              " u'behance',\n",
              " u'behave',\n",
              " u'behaving',\n",
              " u'behavior',\n",
              " u'behind',\n",
              " u'being',\n",
              " u'believe',\n",
              " u'belinsky',\n",
              " u'belong',\n",
              " u'beluga',\n",
              " u'ben',\n",
              " u'benefit',\n",
              " u'benieuwd',\n",
              " u'bereft',\n",
              " u'bergstrom',\n",
              " u'berklee',\n",
              " u'bernd',\n",
              " u'berry',\n",
              " u'best',\n",
              " u'bestappever',\n",
              " u'bestie',\n",
              " u'bet',\n",
              " u'beta',\n",
              " u'betainvites',\n",
              " u'better',\n",
              " u'bettersearch',\n",
              " u'betterthingstodo',\n",
              " u'between',\n",
              " u'beware',\n",
              " u'beyond',\n",
              " u'bff',\n",
              " u'bicycle',\n",
              " u'big',\n",
              " u'bigger',\n",
              " u'biggest',\n",
              " u'bike',\n",
              " u'billion',\n",
              " u'bin',\n",
              " u'bing',\n",
              " u'biomimicry',\n",
              " u'bird',\n",
              " u'birds',\n",
              " u'birth',\n",
              " u'bit',\n",
              " u'bite',\n",
              " u'biz',\n",
              " u'bizzy',\n",
              " u'bjdproductions',\n",
              " u'black',\n",
              " u'blackberry',\n",
              " u'blackbook',\n",
              " u'blacked',\n",
              " u'blame',\n",
              " u'blast',\n",
              " u'bleed',\n",
              " u'blew',\n",
              " u'blind',\n",
              " u'blinksale',\n",
              " u'block',\n",
              " u'blocked',\n",
              " u'blocks',\n",
              " u'blog',\n",
              " u'bloggable',\n",
              " u'blogger',\n",
              " u'blogging',\n",
              " u'blogs',\n",
              " u'bloody',\n",
              " u'bloomberg',\n",
              " u'blows',\n",
              " u'blue',\n",
              " u'blueray',\n",
              " u'bluetooth',\n",
              " u'bluezoom',\n",
              " u'bmm',\n",
              " u'bnet',\n",
              " u'board',\n",
              " u'boarded',\n",
              " u'body',\n",
              " u'bomb',\n",
              " u'boo',\n",
              " u'book',\n",
              " u'bookbook',\n",
              " u'books',\n",
              " u'boom',\n",
              " u'boomers',\n",
              " u'boost',\n",
              " u'booth',\n",
              " u'boots',\n",
              " u'booyah',\n",
              " u'booze',\n",
              " u'borderstylo',\n",
              " u'bored',\n",
              " u'born',\n",
              " u'borrow',\n",
              " u'borrowing',\n",
              " u'boss',\n",
              " u'botch',\n",
              " u'both',\n",
              " u'bother',\n",
              " u'bots',\n",
              " u'bottom',\n",
              " u'bought',\n",
              " u'bounced',\n",
              " u'bound',\n",
              " u'boundaries',\n",
              " u'bout',\n",
              " u'bowl',\n",
              " u'box',\n",
              " u'boxee',\n",
              " u'boxes',\n",
              " u'boy',\n",
              " u'boyfriend',\n",
              " u'boys',\n",
              " u'bpm',\n",
              " u'bracket',\n",
              " u'brah',\n",
              " u'brain',\n",
              " u'brains',\n",
              " u'brainwashed',\n",
              " u'brand',\n",
              " u'branded',\n",
              " u'brands',\n",
              " u'bravo',\n",
              " u'brawls',\n",
              " u'brazil',\n",
              " u'bread',\n",
              " u'break',\n",
              " u'breakdown',\n",
              " u'breakfast',\n",
              " u'breaking',\n",
              " u'breakout',\n",
              " u'breakthrough',\n",
              " u'breathtaking',\n",
              " u'breeds',\n",
              " u'brian_lam',\n",
              " u'brick',\n",
              " u'bricklin',\n",
              " u'bridging',\n",
              " u'bright',\n",
              " u'brightens',\n",
              " u'brightness',\n",
              " u'brilliance',\n",
              " u'brilliant',\n",
              " u'bring',\n",
              " u'bringing',\n",
              " u'brings',\n",
              " u'brisk',\n",
              " u'british',\n",
              " u'brits',\n",
              " u'brk',\n",
              " u'bro',\n",
              " u'broadcast',\n",
              " u'broadcastr',\n",
              " u'broadfeed',\n",
              " u'broken',\n",
              " u'brother',\n",
              " u'brought',\n",
              " u'browse',\n",
              " u'browser',\n",
              " u'browserwars',\n",
              " u'browsing',\n",
              " u'bruises',\n",
              " u'brushstroke',\n",
              " u'bryce',\n",
              " u'bt',\n",
              " u'btw',\n",
              " u'bubble',\n",
              " u'bucket',\n",
              " u'buffalo',\n",
              " u'bug',\n",
              " u'bugger',\n",
              " u'buggy',\n",
              " u'bugs',\n",
              " u'build',\n",
              " u'building',\n",
              " u'buildings',\n",
              " u'built',\n",
              " u'bulletin',\n",
              " u'bummed',\n",
              " u'bummer',\n",
              " u'bumped',\n",
              " u'bunch',\n",
              " u'burn',\n",
              " u'bursts',\n",
              " u'bus',\n",
              " u'busdev',\n",
              " u'business',\n",
              " u'businesses',\n",
              " u'busy',\n",
              " u'but',\n",
              " u'butt',\n",
              " u'button',\n",
              " u'buttons',\n",
              " u'butts',\n",
              " u'buy',\n",
              " u'buyers',\n",
              " u'buying',\n",
              " u'buys',\n",
              " u'buzz',\n",
              " u'buzzing',\n",
              " u'by',\n",
              " u'bynd',\n",
              " u'ca',\n",
              " u'cab',\n",
              " u'cabbies',\n",
              " u'cable',\n",
              " u'cables',\n",
              " u'cabs',\n",
              " u'cactus',\n",
              " u'cake',\n",
              " u'calendar',\n",
              " u'calhoun',\n",
              " u'california',\n",
              " u'call',\n",
              " u'callback',\n",
              " u'called',\n",
              " u'calls',\n",
              " u'calyp',\n",
              " u'cam',\n",
              " u'came',\n",
              " u'camera',\n",
              " u'cameras',\n",
              " u'campaigns',\n",
              " u'campbell',\n",
              " u'campus',\n",
              " u'can',\n",
              " u'canada',\n",
              " u'canadian',\n",
              " u'cancel',\n",
              " u'cannot',\n",
              " u'cant',\n",
              " u'canvas',\n",
              " u'capabilities',\n",
              " u'capitol',\n",
              " u'capped',\n",
              " u'capture',\n",
              " u'captured',\n",
              " u'car',\n",
              " u'caramel',\n",
              " u'carbon',\n",
              " u'card',\n",
              " u'cards',\n",
              " u'care',\n",
              " u'career',\n",
              " u'caring',\n",
              " u'carry',\n",
              " u'carrying',\n",
              " u'cart',\n",
              " u'cartel',\n",
              " u'cartoon',\n",
              " u'cartoonishly',\n",
              " u'case',\n",
              " u'cases',\n",
              " u'cash',\n",
              " u'cashmere',\n",
              " u'cashmore',\n",
              " u'cast',\n",
              " u'castle',\n",
              " u'casually',\n",
              " u'cat',\n",
              " u'catch',\n",
              " u'catching',\n",
              " u'catphysics',\n",
              " u'cattle',\n",
              " u'cause',\n",
              " u'caused',\n",
              " u'causing',\n",
              " u'cautiously',\n",
              " u'cbatsxsw',\n",
              " u'cc',\n",
              " u'cedar',\n",
              " u'celebrate',\n",
              " u'celebrating',\n",
              " u'celebs',\n",
              " u'cell',\n",
              " u'cellular',\n",
              " u'center',\n",
              " u'centers',\n",
              " u'central',\n",
              " u'centre',\n",
              " u'cents',\n",
              " u'ceo',\n",
              " u'ceokidschat',\n",
              " u'cera',\n",
              " u'cerebellum',\n",
              " u'cerebral',\n",
              " u'certain',\n",
              " u'certificate',\n",
              " u'ces',\n",
              " u'chain',\n",
              " u'chair',\n",
              " u'chalked',\n",
              " u'challenged',\n",
              " u'challenges',\n",
              " u'champ',\n",
              " u'chance',\n",
              " u'chances',\n",
              " u'change',\n",
              " u'changed',\n",
              " u'changer',\n",
              " u'changes',\n",
              " u'changing',\n",
              " u'channel',\n",
              " u'channels',\n",
              " u'chaos',\n",
              " u'characters',\n",
              " u'charge',\n",
              " u'charged',\n",
              " u'charger',\n",
              " u'chargers',\n",
              " u'charges',\n",
              " u'chargin2diffphonesatonce',\n",
              " u'charging',\n",
              " u'charity',\n",
              " u'charles',\n",
              " u'charm',\n",
              " u'charts',\n",
              " u'chat',\n",
              " u'chatter',\n",
              " u'chatting',\n",
              " u'cheapen',\n",
              " u'cheaper',\n",
              " u'check',\n",
              " u'checked',\n",
              " u'checking',\n",
              " u'checkins',\n",
              " u'cheeky',\n",
              " u'cheer',\n",
              " u'cheers',\n",
              " u'cheese',\n",
              " u'chen',\n",
              " u'chevy',\n",
              " u'chevysmc',\n",
              " u'chevysxsw',\n",
              " u'chevytweethouse',\n",
              " u'chic',\n",
              " u'chief',\n",
              " u'childhood',\n",
              " u'chill',\n",
              " u'chilltab',\n",
              " u'china',\n",
              " u'chinese',\n",
              " u'chip',\n",
              " u'chk',\n",
              " u'chng',\n",
              " u'choice',\n",
              " u'chokes',\n",
              " u'choose',\n",
              " u'choplifter',\n",
              " u'choreography',\n",
              " u'chris',\n",
              " u'christian',\n",
              " u'christmas',\n",
              " u'chrome',\n",
              " u'chromeos',\n",
              " u'chronicling',\n",
              " u'chumps',\n",
              " u'chunky',\n",
              " u'cigarettes',\n",
              " u'cinema',\n",
              " u'circle',\n",
              " u'circles',\n",
              " u'circusmash',\n",
              " u'cited',\n",
              " u'cites',\n",
              " u'city',\n",
              " u'ck',\n",
              " u'cks',\n",
              " u'claims',\n",
              " u'clarity',\n",
              " u'clark',\n",
              " u'class',\n",
              " u'classics',\n",
              " u'classiest',\n",
              " u'classy',\n",
              " u'cle',\n",
              " u'clean',\n",
              " u'clear',\n",
              " u'clearly',\n",
              " u'cleveland',\n",
              " u'clever',\n",
              " u'click',\n",
              " u'clicked',\n",
              " u'client',\n",
              " u'clients',\n",
              " u'climbing',\n",
              " u'clipcon',\n",
              " u'clocks',\n",
              " u'close',\n",
              " u'closed',\n",
              " u'closely',\n",
              " u'closer',\n",
              " u'clothes',\n",
              " u'cloud',\n",
              " u'cloudapp',\n",
              " u'cloudsight',\n",
              " u'clumsily',\n",
              " u'cluster',\n",
              " u'cluttering',\n",
              " u'cm48',\n",
              " u'cmswire',\n",
              " u'cmty',\n",
              " u'cnet',\n",
              " u'cnn',\n",
              " u'cnngrill',\n",
              " u'cnnmoneysxsw',\n",
              " u'cnt',\n",
              " u'cntr',\n",
              " u'co',\n",
              " u'cobra',\n",
              " u'cocaine',\n",
              " u'cocky',\n",
              " u'cocoon',\n",
              " u'code',\n",
              " u'coded',\n",
              " u'coders',\n",
              " u'coding',\n",
              " u'coffee',\n",
              " u'cohen',\n",
              " u'coincides',\n",
              " u'cold',\n",
              " u'colin',\n",
              " u'collab',\n",
              " u'collection',\n",
              " u'collective',\n",
              " u'collectively',\n",
              " u'color',\n",
              " u'colors',\n",
              " u'colour',\n",
              " u'com',\n",
              " u'combine',\n",
              " u'combines',\n",
              " u'comcom',\n",
              " u'come',\n",
              " u'comedy',\n",
              " u'comers',\n",
              " u'comes',\n",
              " u'comfort',\n",
              " u'comfortable',\n",
              " u'coming',\n",
              " u'commandeered',\n",
              " u'comment',\n",
              " u'comments',\n",
              " u'common',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwtgjTBeH0Ny",
        "colab_type": "text"
      },
      "source": [
        "#### Tip: To see all available functions for an Object use dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShA6D8jKH0N5",
        "colab_type": "text"
      },
      "source": [
        "### 18. Find out how many Positive and Negative emotions are there.\n",
        "\n",
        "Hint: Use value_counts on that column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7LAl5pzH0N6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ad0b56c1-1bbe-41f7-80f7-c865f0fcfd0c"
      },
      "source": [
        "df.is_there_an_emotion_directed_at_a_brand_or_product.value_counts()"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positive emotion    2672\n",
              "Negative emotion     519\n",
              "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUvgj0FoH0N9",
        "colab_type": "text"
      },
      "source": [
        "### 19. Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'Label'\n",
        "\n",
        "Hint: use map on that column and give labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaRafp8neCU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_map = { 'Positive emotion' : '1', 'Negative emotion' : '0'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YftKwFv7H0N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"label\"] = df.is_there_an_emotion_directed_at_a_brand_or_product.map(label_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YErwYLCH0N_",
        "colab_type": "text"
      },
      "source": [
        "### 20. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YmpOR-JfKgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "112c6283-063b-4f74-de56-d18c06475441"
      },
      "source": [
        "df.sample()"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>emotion_in_tweet_is_directed_at</th>\n",
              "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1178</th>\n",
              "      <td>So I went the whole day w/out my laptop &amp;amp; ...</td>\n",
              "      <td>iPad</td>\n",
              "      <td>Negative emotion</td>\n",
              "      <td>So I went the whole day w/out my laptop &amp;amp; ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             tweet_text  \\\n",
              "1178  So I went the whole day w/out my laptop &amp; ...   \n",
              "\n",
              "     emotion_in_tweet_is_directed_at  \\\n",
              "1178                            iPad   \n",
              "\n",
              "     is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
              "1178                                   Negative emotion   \n",
              "\n",
              "                                                   text label  \n",
              "1178  So I went the whole day w/out my laptop &amp; ...     0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNkwrGgEH0OA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCQosQYRfYXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y =df[[\"label\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnQcp4o0fxdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKDOi8wEf8LF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5nlCuaaH0OD",
        "colab_type": "text"
      },
      "source": [
        "## 21. **Predicting the sentiment:**\n",
        "\n",
        "\n",
        "### Use Naive Bayes and Logistic Regression and their accuracy scores for predicting the sentiment of the given text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktXrLhmOH0Of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clv2X0kKH0Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model =LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K86LRMfdH0Ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bc5752af-41fa-46f2-dcfb-b61e1581f463"
      },
      "source": [
        "model.fit(x_train,y_train)"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 306
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6sqNCc1lVpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Uauc7ym5qU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4ba51ec-bd5d-402c-e722-46910044166e"
      },
      "source": [
        "from sklearn import metrics\n",
        "metrics.accuracy_score(y_test,pred)"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8785578747628083"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEmOAEIjnmu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgmeGNB4n3RD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = MultinomialNB()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwmTU4bun3a_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8790c64f-3d98-4738-89e7-a6bf09a9e144"
      },
      "source": [
        "model1.fit(x_train.toarray(),y_train)"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnRS0xVPoRxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred1 = model1.predict(x_test.toarray())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGgnyaBpoVxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28983baa-e95b-436f-c8bd-5d15d01a6526"
      },
      "source": [
        "metrics.accuracy_score(y_test,pred1)"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8671726755218216"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 313
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw-0B33tH0Ox",
        "colab_type": "text"
      },
      "source": [
        "## 22. Create a function called `tokenize_predict` which can take count vectorizer object as input and prints the accuracy for x (text) and y (labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUxTXvNZpmD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_predict(vect):\n",
        "    X =  vect.fit_transform(df.text)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)\n",
        "    print('Features: ', x_train.shape[1])\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(x_train, y_train)\n",
        "    y_pred_class = nb.predict(x_test)\n",
        "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxZ8jfPEH0O0",
        "colab_type": "text"
      },
      "source": [
        "### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezpVBG66rfPE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "30e2a60d-1aa5-48dc-f91d-c331d13c2343"
      },
      "source": [
        "vect = CountVectorizer(ngram_range=(1,2))\n",
        "tokenize_predict(vect)"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Features: ', 28958)\n",
            "('Accuracy: ', 0.8652751423149905)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axepytmgH0O4",
        "colab_type": "text"
      },
      "source": [
        "### Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HToGkq7vH0O4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4f93a503-e9af-432f-d59c-3a36bebba405"
      },
      "source": [
        "vect = CountVectorizer(stop_words='english')\n",
        "tokenize_predict(vect)"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Features: ', 5239)\n",
            "('Accuracy: ', 0.8652751423149905)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOIlJRxoH0O7",
        "colab_type": "text"
      },
      "source": [
        "### Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fUhff-oH0O8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "545f666e-c58d-4df9-cc7d-650d37cfb657"
      },
      "source": [
        "vect = CountVectorizer(stop_words='english',max_features=300)\n",
        "tokenize_predict(vect)"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Features: ', 300)\n",
            "('Accuracy: ', 0.8358633776091081)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2KZNWVkH0PA",
        "colab_type": "text"
      },
      "source": [
        "### Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v9XD082H0PB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "371fb668-833d-405c-c8ff-6801af13cfa4"
      },
      "source": [
        "vect = CountVectorizer(ngram_range=(1,2),max_features=15000)\n",
        "tokenize_predict(vect)"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Features: ', 15000)\n",
            "('Accuracy: ', 0.8595825426944972)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We3JK_SRH0PO",
        "colab_type": "text"
      },
      "source": [
        "### Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUHrfDCyH0PP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aaba9fa7-cf4b-483f-a0c8-883bc5fd9f2e"
      },
      "source": [
        "vect = CountVectorizer(ngram_range=(1,2),min_df=2)\n",
        "tokenize_predict(vect)"
      ],
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Features: ', 9914)\n",
            "('Accuracy: ', 0.8425047438330171)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H4k_lVZH0PS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}